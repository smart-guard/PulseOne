// collector/src/Pipeline/WorkerPipelineManager.cpp
#include "Pipeline/WorkerPipelineManager.h"
#include "Utils/LogManager.h"
#include <algorithm>
#include <chrono>
#include <sstream>
#include <iomanip>

namespace PulseOne {
namespace Pipeline {

// =============================================================================
// ìƒì„±ì ë° ì†Œë©¸ì
// =============================================================================

WorkerPipelineManager::WorkerPipelineManager(
    std::shared_ptr<RedisClient> redis_client,
    std::shared_ptr<InfluxClient> influx_client,
    std::shared_ptr<RabbitMQClient> rabbitmq_client)
    : redis_client_(redis_client)
    , influx_client_(influx_client)
    , rabbitmq_client_(rabbitmq_client) {
    
    // ê¸°ë³¸ ìŠ¤ë ˆë“œ ìˆ˜ë¥¼ CPU ì½”ì–´ ìˆ˜ë¡œ ì„¤ì • (ìµœì†Œ 2ê°œ, ìµœëŒ€ 8ê°œ)
    thread_count_ = std::max(2u, std::min(8u, std::thread::hardware_concurrency()));
    
    LogManager::getInstance().Info("WorkerPipelineManager created with {} threads", thread_count_);
}

WorkerPipelineManager::~WorkerPipelineManager() {
    Stop();
    LogManager::getInstance().Info("WorkerPipelineManager destroyed");
}

// =============================================================================
// ğŸ”¥ ê³µê°œ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
// =============================================================================

bool WorkerPipelineManager::ProcessDeviceData(
    const std::string& device_id,
    const std::vector<Structs::TimestampedValue>& values,
    uint32_t priority) {
    
    if (values.empty() || !is_running_.load()) {
        return false;
    }
    
    // ğŸ”¥ DeviceDataMessage ìƒì„±
    Structs::DeviceDataMessage message;
    message.device_id = device_id;
    message.protocol = "AUTO_DETECTED";  // Workerì—ì„œ ì„¤ì •í•˜ë„ë¡ ê°œì„  ê°€ëŠ¥
    message.points = values;
    message.priority = priority;
    
    // ğŸ”¥ ë¼ìš´ë“œ ë¡œë¹ˆìœ¼ë¡œ ìŠ¤ë ˆë“œ ì„ íƒ
    size_t thread_index = round_robin_counter_.fetch_add(1) % thread_count_;
    
    {
        std::lock_guard<std::mutex> lock(thread_mutexes_[thread_index]);
        
        // í ì˜¤ë²„í”Œë¡œìš° ì²´í¬
        if (thread_queues_[thread_index].size() >= max_queue_size_ / thread_count_) {
            stats_.total_dropped.fetch_add(1);
            LogManager::getInstance().Warn("Pipeline queue {} overflow, dropping data for device: {}", 
                                         thread_index, device_id);
            return false;
        }
        
        // ğŸ”¥ DeviceDataMessageë¥¼ íì— ì¶”ê°€
        thread_queues_[thread_index].push(message);
    }
    
    // í•´ë‹¹ ìŠ¤ë ˆë“œ ê¹¨ìš°ê¸°
    thread_cvs_[thread_index].notify_one();
    
    // í†µê³„ ì—…ë°ì´íŠ¸
    UpdateQueueSizeStats();
    
    return true;
}

bool WorkerPipelineManager::Start() {
    if (is_running_.load()) {
        LogManager::getInstance().Warn("WorkerPipelineManager already running");
        return false;
    }
    
    LogManager::getInstance().Info("Starting WorkerPipelineManager with {} threads...", thread_count_);
    
    // ğŸ”¥ ë©€í‹°ìŠ¤ë ˆë“œ í ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    thread_queues_.resize(thread_count_);
    thread_mutexes_.resize(thread_count_);
    thread_cvs_.resize(thread_count_);
    processing_threads_.reserve(thread_count_);
    
    is_running_ = true;
    
    // ğŸ”¥ ì²˜ë¦¬ ìŠ¤ë ˆë“œë“¤ ì‹œì‘
    for (size_t i = 0; i < thread_count_; ++i) {
        processing_threads_.emplace_back(&WorkerPipelineManager::ProcessingThreadLoop, this, i);
        LogManager::getInstance().Debug("Started processing thread {}", i);
    }
    
    // í†µê³„ ì´ˆê¸°í™”
    stats_.start_time = std::chrono::system_clock::now();
    
    LogManager::getInstance().Info("WorkerPipelineManager started successfully");
    return true;
}

void WorkerPipelineManager::Stop() {
    if (!is_running_.load()) {
        return;
    }
    
    LogManager::getInstance().Info("Stopping WorkerPipelineManager...");
    
    // ğŸ”¥ ëª¨ë“  ìŠ¤ë ˆë“œì— ì¢…ë£Œ ì‹ í˜¸
    is_running_ = false;
    
    // ëª¨ë“  condition variable ê¹¨ìš°ê¸°
    for (auto& cv : thread_cvs_) {
        cv.notify_all();
    }
    
    // ğŸ”¥ ëª¨ë“  ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
    for (size_t i = 0; i < processing_threads_.size(); ++i) {
        if (processing_threads_[i].joinable()) {
            processing_threads_[i].join();
            LogManager::getInstance().Debug("Stopped processing thread {}", i);
        }
    }
    
    // ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    processing_threads_.clear();
    thread_queues_.clear();
    thread_mutexes_.clear();
    thread_cvs_.clear();
    
    LogManager::getInstance().Info("WorkerPipelineManager stopped");
}

void WorkerPipelineManager::SetThreadCount(size_t thread_count) {
    if (is_running_.load()) {
        LogManager::getInstance().Warn("Cannot change thread count while running");
        return;
    }
    
    thread_count_ = std::max(1u, std::min(16u, static_cast<unsigned>(thread_count)));
    LogManager::getInstance().Info("Thread count set to {}", thread_count_);
}

void WorkerPipelineManager::ResetStatistics() {
    stats_.total_processed = 0;
    stats_.total_dropped = 0;
    stats_.redis_writes = 0;
    stats_.influx_writes = 0;
    stats_.rabbitmq_publishes = 0;
    stats_.alarm_events = 0;
    stats_.high_priority_events = 0;
    stats_.current_queue_size = 0;
    stats_.avg_processing_time_ms = 0.0;
    stats_.start_time = std::chrono::system_clock::now();
    
    LogManager::getInstance().Info("Pipeline statistics reset");
}

// =============================================================================
// ğŸ”¥ ë©€í‹°ìŠ¤ë ˆë“œ ì²˜ë¦¬ ë¡œì§
// =============================================================================

void WorkerPipelineManager::ProcessingThreadLoop(size_t thread_index) {
    LogManager::getInstance().Info("Pipeline processing thread {} started", thread_index);
    
    while (is_running_.load()) {
        std::vector<Structs::DeviceDataMessage> batch;
        batch.reserve(batch_size_);
        
        // ğŸ”¥ íì—ì„œ ë°°ì¹˜ ë°ì´í„° ìˆ˜ì§‘
        {
            std::unique_lock<std::mutex> lock(thread_mutexes_[thread_index]);
            
            // ë°ì´í„°ê°€ ìˆê±°ë‚˜ ì¢…ë£Œ ì‹ í˜¸ê¹Œì§€ ëŒ€ê¸°
            thread_cvs_[thread_index].wait(lock, [this, thread_index] {
                return !thread_queues_[thread_index].empty() || !is_running_.load();
            });
            
            // ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë˜ëŠ” íê°€ ë¹Œ ë•Œê¹Œì§€ ìˆ˜ì§‘
            while (!thread_queues_[thread_index].empty() && batch.size() < batch_size_) {
                batch.push_back(std::move(thread_queues_[thread_index].front()));
                thread_queues_[thread_index].pop();
            }
        }
        
        // ğŸ”¥ ë°°ì¹˜ ì²˜ë¦¬ (ë½ í•´ì œ í›„)
        if (!batch.empty()) {
            ProcessBatch(batch, thread_index);
        }
    }
    
    LogManager::getInstance().Info("Pipeline processing thread {} stopped", thread_index);
}

void WorkerPipelineManager::ProcessBatch(const std::vector<Structs::DeviceDataMessage>& batch, size_t thread_index) {
    auto start_time = std::chrono::steady_clock::now();
    
    for (const auto& message : batch) {
        try {
            // ğŸ”¥ 1. Redisì— ìµœì‹  ê°’ ì €ì¥ (DeviceDataMessage í™œìš©)
            WriteToRedis(message);
            stats_.redis_writes.fetch_add(1);
            
            // ğŸ”¥ 2. InfluxDBì— ì‹œê³„ì—´ ì €ì¥
            WriteToInflux(message);
            stats_.influx_writes.fetch_add(1);
            
            // ğŸ”¥ 3. ê° ê°’ì— ëŒ€í•´ ì•ŒëŒ ì²´í¬
            for (const auto& point : message.points) {
                // ì•ŒëŒ ì¡°ê±´ ì²´í¬
                if (CheckAlarmCondition(message.device_id, point)) {
                    PublishAlarmToRabbitMQ(message.device_id, point, "HIGH");
                    stats_.alarm_events.fetch_add(1);
                }
                
                // ë†’ì€ ìš°ì„ ìˆœìœ„ ë°ì´í„° ì²´í¬
                if (message.priority > 0 || IsHighPriorityData(point)) {
                    // ì¤‘ìš” ë°ì´í„°ëŠ” RabbitMQë¡œ ì‹¤ì‹œê°„ ì•Œë¦¼
                    PublishAlarmToRabbitMQ(message.device_id, point, "PRIORITY");
                    stats_.high_priority_events.fetch_add(1);
                }
            }
            
            stats_.total_processed.fetch_add(message.points.size());
            
        } catch (const std::exception& e) {
            LogManager::getInstance().Error("Thread {} processing error for device {}: {}", 
                                          thread_index, message.device_id, e.what());
        }
    }
    
    // ì²˜ë¦¬ ì‹œê°„ í†µê³„ ì—…ë°ì´íŠ¸
    auto end_time = std::chrono::steady_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);
    double processing_time_ms = static_cast<double>(duration.count());
    
    UpdateProcessingTimeStats(processing_time_ms);
    
    LogManager::getInstance().Debug("Thread {} processed batch of {} messages in {}ms", 
                                   thread_index, batch.size(), processing_time_ms);
}

// =============================================================================
// ğŸ”¥ ë°ì´í„° ì €ì¥ êµ¬í˜„
// =============================================================================

void WorkerPipelineManager::WriteToRedis(const Structs::DeviceDataMessage& message) {
    if (!redis_client_) return;
    
    try {
        // ğŸ”¥ DeviceDataMessageì˜ ToJSON() ë©”ì„œë“œ í™œìš©
        std::string json_data = message.ToJSON();
        
        // ë””ë°”ì´ìŠ¤ë³„ ìµœì‹  ë°ì´í„° ì €ì¥
        std::string key = "device:" + message.device_id + ":latest";
        redis_client_->Set(key, json_data);
        
        // ê°œë³„ í¬ì¸íŠ¸ë³„ë¡œë„ ì €ì¥ (ë¹ ë¥¸ ì¡°íšŒìš©)
        for (const auto& point : message.points) {
            std::string point_key = "device:" + message.device_id + ":point:" + point.point_id;
            std::string point_json = point.ToJSON();  // TimestampedValueì˜ ToJSON ì‚¬ìš©
            redis_client_->Set(point_key, point_json);
        }
        
        // ë””ë°”ì´ìŠ¤ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„
        std::string last_update_key = "device:" + message.device_id + ":last_update";
        auto timestamp = std::chrono::system_clock::now().time_since_epoch().count();
        redis_client_->Set(last_update_key, std::to_string(timestamp));
        
    } catch (const std::exception& e) {
        LogManager::getInstance().Error("Redis write error for device {}: {}", message.device_id, e.what());
    }
}

void WorkerPipelineManager::WriteToInflux(const Structs::DeviceDataMessage& message) {
    if (!influx_client_) return;
    
    try {
        for (const auto& point : message.points) {
            // InfluxDB ë¼ì¸ í”„ë¡œí† ì½œ ìƒì„±
            std::ostringstream line;
            line << "device_data,";
            line << "device_id=" << message.device_id << ",";
            line << "protocol=" << message.protocol << ",";
            line << "point_id=" << point.point_id << " ";
            
            // variant ê°’ ì²˜ë¦¬
            std::visit([&line](const auto& v) {
                line << "value=" << v;
            }, point.value);
            
            line << " ";
            
            // ë‚˜ë…¸ì´ˆ íƒ€ì„ìŠ¤íƒ¬í”„
            auto timestamp_ns = std::chrono::duration_cast<std::chrono::nanoseconds>(
                point.timestamp.time_since_epoch()).count();
            line << timestamp_ns;
            
            influx_client_->Write(line.str());
        }
    } catch (const std::exception& e) {
        LogManager::getInstance().Error("InfluxDB write error for device {}: {}", message.device_id, e.what());
    }
}

// =============================================================================
// ğŸ”¥ ì•ŒëŒ ë° ì´ë²¤íŠ¸ ì²˜ë¦¬
// =============================================================================

bool WorkerPipelineManager::CheckAlarmCondition(
    const std::string& device_id, 
    const Structs::TimestampedValue& value) {
    
    // ì„ê³„ê°’ ì²´í¬
    auto threshold_key = device_id + ":" + value.point_id;
    auto it = alarm_thresholds_.find(threshold_key);
    if (it != alarm_thresholds_.end()) {
        return value.value.numeric_value > it->second;
    }
    
    // ê¸°ë³¸ ì„ê³„ê°’ë“¤
    if (value.point_id.find("temperature") != std::string::npos) {
        return value.value.numeric_value > 80.0;  // ì˜¨ë„ 80ë„ ì´ˆê³¼
    }
    if (value.point_id.find("pressure") != std::string::npos) {
        return value.value.numeric_value > 10.0;  // ì••ë ¥ 10 ì´ˆê³¼
    }
    
    return false;
}

void WorkerPipelineManager::PublishAlarmToRabbitMQ(
    const std::string& device_id,
    const Structs::TimestampedValue& point,
    const std::string& severity) {
    
    if (!rabbitmq_client_) return;
    
    try {
        // ğŸ”¥ Structs::AlarmEvent ì‚¬ìš©
        Structs::AlarmEvent alarm;
        alarm.device_id = device_id;
        alarm.point_id = point.point_id;
        alarm.current_value = point.value;
        alarm.severity = severity;
        alarm.timestamp = point.timestamp;
        alarm.message = "Value exceeded threshold";
        
        // ğŸ”¥ AlarmEventì˜ ToJSON() ë©”ì„œë“œ í™œìš©
        std::string json_alarm = alarm.ToJSON();
        
        // RabbitMQì— ë°œí–‰
        std::string exchange = "alarms";
        std::string routing_key = severity == "CRITICAL" ? "alarm.critical" : "alarm.normal";
        
        rabbitmq_client_->Publish(exchange, routing_key, json_alarm);
        stats_.rabbitmq_publishes.fetch_add(1);
        
        LogManager::getInstance().Info("Published {} alarm for device {} point {}", 
                                     severity, device_id, point.point_id);
        
    } catch (const std::exception& e) {
        LogManager::getInstance().Error("RabbitMQ publish error: {}", e.what());
    }
}

bool WorkerPipelineManager::IsHighPriorityData(const Structs::TimestampedValue& point) {
    // íŠ¹ì • í¬ì¸íŠ¸ë“¤ì€ í•­ìƒ ë†’ì€ ìš°ì„ ìˆœìœ„
    return point.point_id.find("critical") != std::string::npos ||
           point.point_id.find("emergency") != std::string::npos ||
           point.point_id.find("alarm") != std::string::npos;
}

// =============================================================================
// ğŸ”¥ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤ (SerializeAlarmEvent, SerializeTimestampedValue ì œê±°)
// =============================================================================

void WorkerPipelineManager::UpdateProcessingTimeStats(double processing_time_ms) {
    // ì´ë™ í‰ê· ìœ¼ë¡œ ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
    double current_avg = stats_.avg_processing_time_ms.load();
    double new_avg = (current_avg * 0.95) + (processing_time_ms * 0.05);
    stats_.avg_processing_time_ms.store(new_avg);
}

void WorkerPipelineManager::UpdateQueueSizeStats() {
    size_t total_queue_size = 0;
    for (size_t i = 0; i < thread_count_; ++i) {
        std::lock_guard<std::mutex> lock(thread_mutexes_[i]);
        total_queue_size += thread_queues_[i].size();
    }
    stats_.current_queue_size.store(total_queue_size);
}

} // namespace Pipeline
} // namespace PulseOne